{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, test and val folders\n",
    "# Todo: codice per lo split del dataset in train, val e test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image dataset transforms\n",
    "The `torchvision.transforms` module includes additional classes specific for image pre-processing. Some of them are:\n",
    "\n",
    "- `Resize`: resizes an image;\n",
    "- `RandomCrop`: randomly crops an image (data augmentation during training);\n",
    "- `RandomHorizontalFlip`: randomly flips an image (data augmentation during training);\n",
    "- `CenterCrop`: crops the central area of an image (used in testing, as counterpart to `RandomCrop`);\n",
    "- `Normalize`: performs standardization, given per-channel means and standard deviations.\n",
    "\n",
    "Usually, to do data augmentation, you crop an image to an area which is slightly smaller than the full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "resize = T.Resize(32)  \n",
    "random_crop = T.RandomCrop(28)             # train\n",
    "random_hor_flip = T.RandomHorizontalFlip()  # train\n",
    "center_crop = T.CenterCrop(28)             # test and val\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
    "\n",
    "# Compose transforms\n",
    "train_transform = T.Compose([resize, random_crop, random_hor_flip, to_tensor, normalize])\n",
    "test_transform = T.Compose([resize, center_crop, to_tensor, normalize])\n",
    "val_transform = T.Compose([resize, center_crop, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. classes: 2\n",
      "Num. train samples: 70000\n",
      "Num. valid. samples: 10000\n",
      "Num. test samples: 20000\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets\n",
    "root_dir = \"../datasets/\"\n",
    "train_dataset = ImageFolder(os.path.join(root_dir, \"black_white\", \"train\"), transform=train_transform) \n",
    "val_dataset = ImageFolder(os.path.join(root_dir, \"black_white\", \"valid\"), transform=val_transform)\n",
    "test_dataset = ImageFolder(os.path.join(root_dir, \"black_white\", \"test\"), transform=test_transform) \n",
    "\n",
    "# Get number of classes (we'll need it in the model)\n",
    "num_classes = len(train_dataset.classes)\n",
    "batch_size = 64\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Num. classes: {num_classes}\")\n",
    "print(f\"Num. train samples: {len(train_dataset)}\")\n",
    "print(f\"Num. valid. samples: {len(val_dataset)}\")\n",
    "print(f\"Num. test samples: {len(test_dataset)}\")\n",
    "\n",
    "# def loader(path):\n",
    "#    print(path)\n",
    "#    return PIL.Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image size (\"size\" is a property of PIL.Image)\n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7096,  0.6778,  0.6778,  ...,  1.0271,  1.3129,  1.4875],\n",
       "         [ 0.5826,  0.6461,  0.6461,  ...,  0.8525,  1.2018,  1.4082],\n",
       "         [ 0.6143,  0.5667,  0.2650,  ...,  0.6620,  1.0747,  1.3129],\n",
       "         ...,\n",
       "         [-0.6241, -0.2113,  0.2333,  ...,  1.6781,  1.7257,  1.7257],\n",
       "         [-0.7035, -0.4335, -0.0525,  ...,  1.5987,  1.5193,  1.4399],\n",
       "         [-0.7352, -0.6241, -0.3383,  ..., -0.4812, -0.8146, -0.9575]],\n",
       "\n",
       "        [[ 0.7591,  0.7268,  0.7268,  ...,  1.0819,  1.3724,  1.5499],\n",
       "         [ 0.6300,  0.6946,  0.6946,  ...,  0.9044,  1.2594,  1.4692],\n",
       "         [ 0.6623,  0.6139,  0.3073,  ...,  0.7107,  1.1303,  1.3724],\n",
       "         ...,\n",
       "         [-0.5965, -0.1769,  0.2750,  ...,  1.7435,  1.7920,  1.7920],\n",
       "         [-0.6772, -0.4028, -0.0155,  ...,  1.6629,  1.5822,  1.5015],\n",
       "         [-0.7094, -0.5965, -0.3060,  ..., -0.4512, -0.7901, -0.9354]],\n",
       "\n",
       "        [[ 0.8436,  0.8135,  0.8135,  ...,  1.1441,  1.4145,  1.5798],\n",
       "         [ 0.7233,  0.7834,  0.7834,  ...,  0.9788,  1.3093,  1.5047],\n",
       "         [ 0.7534,  0.7083,  0.4228,  ...,  0.7985,  1.1891,  1.4145],\n",
       "         ...,\n",
       "         [-0.4186, -0.0279,  0.3928,  ...,  1.7601,  1.8052,  1.8052],\n",
       "         [-0.4937, -0.2383,  0.1223,  ...,  1.6850,  1.6098,  1.5347],\n",
       "         [-0.5237, -0.4186, -0.1481,  ..., -0.2833, -0.5989, -0.7341]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an image of a given class\n",
    "train_dataset[train_dataset.targets.index(1)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Call parent constructor\n",
    "        super().__init__();\n",
    "        # Create convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Create fully-connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # FC layer\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            # Classification layer\n",
    "            nn.Linear(1024, 2)\n",
    "        )\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available? True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, dev, lr=0.001):\n",
    "    try:\n",
    "        # Create model\n",
    "        model = CNN()\n",
    "        model = model.to(dev)\n",
    "        print(model)\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                # Process each batch\n",
    "                for i,(input, labels) in enumerate(loaders[split]):\n",
    "                    # Move to CUDA\n",
    "                    input = input.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = model(input)\n",
    "                    loss = F.cross_entropy(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    _,pred_labels = pred.max(1)\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "                    \n",
    "                    # print batch info\n",
    "                    #if(i%10 == 0):\n",
    "                    #    print(f\"Batch number: {i} | Loss: {loss}\")\n",
    "                    \n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1: TrL=0.6186, TrA=0.6598, VL=0.6118, VA=0.6756, TeL=0.5990, TeA=0.6853,\n",
      "Epoch 2: TrL=0.5881, TrA=0.6928, VL=0.5817, VA=0.7030, TeL=0.5721, TeA=0.7088,\n",
      "Epoch 3: TrL=0.5764, TrA=0.7041, VL=0.5719, VA=0.7098, TeL=0.5623, TeA=0.7157,\n",
      "Epoch 4: TrL=0.5643, TrA=0.7139, VL=0.5607, VA=0.7121, TeL=0.5499, TeA=0.7230,\n",
      "Epoch 5: TrL=0.5513, TrA=0.7232, VL=0.5534, VA=0.7204, TeL=0.5422, TeA=0.7286,\n",
      "Epoch 6: TrL=0.5344, TrA=0.7335, VL=0.5203, VA=0.7412, TeL=0.5054, TeA=0.7510,\n",
      "Epoch 7: TrL=0.5196, TrA=0.7420, VL=0.5437, VA=0.7243, TeL=0.5325, TeA=0.7318,\n",
      "Epoch 8: TrL=0.5037, TrA=0.7528, VL=0.5304, VA=0.7330, TeL=0.5225, TeA=0.7394,\n",
      "Epoch 9: TrL=0.4893, TrA=0.7611, VL=0.4680, VA=0.7806, TeL=0.4607, TeA=0.7839,\n",
      "Epoch 10: TrL=0.4771, TrA=0.7679, VL=0.4630, VA=0.7775, TeL=0.4515, TeA=0.7850,\n",
      "Epoch 11: TrL=0.4636, TrA=0.7767, VL=0.5506, VA=0.7276, TeL=0.5393, TeA=0.7284,\n",
      "Epoch 12: TrL=0.4534, TrA=0.7826, VL=0.4334, VA=0.7971, TeL=0.4223, TeA=0.8017,\n"
     ]
    }
   ],
   "source": [
    "train(100, dev, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
