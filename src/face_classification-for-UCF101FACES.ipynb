{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import shutil\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class map\n",
    "class_names = {0: 'Black', 1: 'White'}\n",
    "\n",
    "# Split the dataset into train, test and val folders\n",
    "args = {\"source\": \"../datasets/black_white\", \"dest\": \"../datasets/black_white_splitted\", \"val_size\": 0.1, \"train_size\": 0.7, \"test_size\": 0.2}\n",
    "\n",
    "# Crea le cartelle di destinazione (se non esistono)\n",
    "if not os.path.exists(args[\"dest\"]):\n",
    "    os.makedirs(args[\"dest\"])\n",
    "\n",
    "dest_train = os.path.join(args[\"dest\"], \"train\")\n",
    "if not os.path.exists(dest_train):\n",
    "    os.makedirs(dest_train)\n",
    "\n",
    "dest_test = os.path.join(args[\"dest\"], \"test\")\n",
    "if not os.path.exists(dest_test):\n",
    "    os.makedirs(dest_test)\n",
    "\n",
    "dest_val = os.path.join(args[\"dest\"], \"val\")\n",
    "if not os.path.exists(dest_val):\n",
    "    os.makedirs(dest_val)\n",
    "\n",
    "class_dirs = os.listdir(args[\"source\"])\n",
    "for class_dir in class_dirs:\n",
    "    class_dir_path = os.path.join(args[\"source\"], class_dir)\n",
    "    num_elem = len(os.listdir(class_dir_path))\n",
    "    total_indices = np.arange(num_elem)\n",
    "    val_indices = np.random.choice(total_indices,\n",
    "                                   size=int(num_elem*args[\"val_size\"]),\n",
    "                                   replace=False)\n",
    "    total_indices = np.delete(total_indices, val_indices)\n",
    "    test_indices = np.random.choice(total_indices,\n",
    "                                   size=int(num_elem*args[\"test_size\"]),\n",
    "                                   replace=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join(dest_train, class_dir)):\n",
    "        os.mkdir(os.path.join(dest_train, class_dir))\n",
    "    if not os.path.exists(os.path.join(dest_test, class_dir)):\n",
    "        os.mkdir(os.path.join(dest_test, class_dir))\n",
    "    if not os.path.exists(os.path.join(dest_val, class_dir)):\n",
    "        os.mkdir(os.path.join(dest_val, class_dir))\n",
    "        \n",
    "    for index, filename in enumerate(os.listdir(class_dir_path)):\n",
    "            if index in val_indices:\n",
    "                shutil.copy(os.path.join(class_dir_path, filename), os.path.join(dest_val, class_dir))\n",
    "            elif index in test_indices:\n",
    "                shutil.copy(os.path.join(class_dir_path, filename), os.path.join(dest_test, class_dir))\n",
    "            else:\n",
    "                shutil.copy(os.path.join(class_dir_path, filename), os.path.join(dest_train, class_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image dataset transforms\n",
    "The `torchvision.transforms` module includes additional classes specific for image pre-processing. Some of them are:\n",
    "\n",
    "- `Resize`: resizes an image;\n",
    "- `RandomCrop`: randomly crops an image (data augmentation during training);\n",
    "- `RandomHorizontalFlip`: randomly flips an image (data augmentation during training);\n",
    "- `CenterCrop`: crops the central area of an image (used in testing, as counterpart to `RandomCrop`);\n",
    "- `Normalize`: performs standardization, given per-channel means and standard deviations.\n",
    "\n",
    "Usually, to do data augmentation, you crop an image to an area which is slightly smaller than the full size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "resize = T.Resize(32)  \n",
    "random_crop = T.RandomCrop(28)             # train\n",
    "random_hor_flip = T.RandomHorizontalFlip()  # train\n",
    "center_crop = T.CenterCrop(28)             # test and val\n",
    "to_tensor = T.ToTensor()\n",
    "normalize = T.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
    "\n",
    "# Compose transforms\n",
    "train_transform = T.Compose([resize, random_crop, random_hor_flip, to_tensor, normalize])\n",
    "test_transform = T.Compose([resize, center_crop, to_tensor, normalize])\n",
    "val_transform = T.Compose([resize, center_crop, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. classes: 2\n",
      "Num. train samples: 100000\n",
      "Num. valid. samples: 68611\n",
      "Num. test samples: 91441\n"
     ]
    }
   ],
   "source": [
    "# Instantiate datasets\n",
    "root_dir = \"../datasets/\"\n",
    "train_dataset = ImageFolder(os.path.join(root_dir, \"black_white_splitted\", \"train\"), transform=train_transform) \n",
    "val_dataset = ImageFolder(os.path.join(root_dir, \"black_white_splitted\", \"val\"), transform=val_transform)\n",
    "test_dataset = ImageFolder(os.path.join(root_dir, \"black_white_splitted\", \"test\"), transform=test_transform) \n",
    "\n",
    "# Get number of classes (we'll need it in the model)\n",
    "num_classes = len(train_dataset.classes)\n",
    "batch_size = 64\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Num. classes: {num_classes}\")\n",
    "print(f\"Num. train samples: {len(train_dataset)}\")\n",
    "print(f\"Num. valid. samples: {len(val_dataset)}\")\n",
    "print(f\"Num. test samples: {len(test_dataset)}\")\n",
    "\n",
    "# def loader(path):\n",
    "#    print(path)\n",
    "#    return PIL.Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# Instantiate data loaders\n",
    "loaders = {\"train\": DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True),\n",
    "           \"val\":   DataLoader(dataset=val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "           \"test\":  DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image size (\"size\" is a property of PIL.Image)\n",
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1004, -1.0369, -0.8305,  ..., -0.7511,  0.6461,  1.3923],\n",
       "         [-1.3385, -1.1004, -0.7193,  ..., -1.1321,  0.3285,  1.1065],\n",
       "         [-1.1639, -0.8463, -0.4971,  ..., -1.2274, -0.6082,  0.6620],\n",
       "         ...,\n",
       "         [-0.6241, -0.3383, -0.1636,  ...,  1.5034,  1.4717,  1.4399],\n",
       "         [-0.7670, -0.5447, -0.4177,  ...,  1.6304,  1.5669,  1.5034],\n",
       "         [-0.7511, -0.5764, -0.5288,  ...,  1.6622,  1.6622,  1.6146]],\n",
       "\n",
       "        [[-1.0806, -1.0161, -0.8063,  ..., -0.7256,  0.6946,  1.4531],\n",
       "         [-1.3227, -1.0806, -0.6933,  ..., -1.1129,  0.3718,  1.1626],\n",
       "         [-1.1452, -0.8224, -0.4674,  ..., -1.2097, -0.5803,  0.7107],\n",
       "         ...,\n",
       "         [-0.5965, -0.3060, -0.1285,  ...,  1.5660,  1.5338,  1.5015],\n",
       "         [-0.7417, -0.5158, -0.3867,  ...,  1.6951,  1.6306,  1.5660],\n",
       "         [-0.7256, -0.5481, -0.4997,  ...,  1.7274,  1.7274,  1.6790]],\n",
       "\n",
       "        [[-0.8693, -0.8092, -0.6139,  ..., -0.5388,  0.7834,  1.4896],\n",
       "         [-1.0947, -0.8693, -0.5087,  ..., -0.8994,  0.4829,  1.2192],\n",
       "         [-0.9294, -0.6289, -0.2984,  ..., -0.9895, -0.4035,  0.7985],\n",
       "         ...,\n",
       "         [-0.4186, -0.1481,  0.0172,  ...,  1.5948,  1.5648,  1.5347],\n",
       "         [-0.5538, -0.3434, -0.2232,  ...,  1.7150,  1.6549,  1.5948],\n",
       "         [-0.5388, -0.3735, -0.3284,  ...,  1.7451,  1.7451,  1.7000]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an image of a given class\n",
    "train_dataset[train_dataset.targets.index(1)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # Call parent constructor\n",
    "        super().__init__();\n",
    "        # Create convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=0, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # Create fully-connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # FC layer\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            # Classification layer\n",
    "            nn.Linear(1024, 2)\n",
    "        )\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available? True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Select device\n",
    "print(f\"CUDA is available? {torch.cuda.is_available()}\")\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, dev, lr=0.001):\n",
    "    try:\n",
    "        # Create model\n",
    "        model = CNN()\n",
    "        model = model.to(dev)\n",
    "        print(model)\n",
    "        # Optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                # Process each batch\n",
    "                for i,(input, labels) in enumerate(loaders[split]):\n",
    "                    # Move to CUDA\n",
    "                    input = input.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = model(input)\n",
    "                    loss = F.cross_entropy(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    _,pred_labels = pred.max(1)\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/input.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "                    \n",
    "                    # print batch info\n",
    "                    #if(i%10 == 0):\n",
    "                    #    print(f\"Batch number: {i} | Loss: {loss}\")\n",
    "                    \n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save(model, \"./models/face_detection_model\" + str(epoch))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1: TrL=0.6085, TrA=0.6717, VL=0.5876, VA=0.6915, TeL=0.5878, TeA=0.6923,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessandro/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: TrL=0.5770, TrA=0.7044, VL=0.5679, VA=0.7131, TeL=0.5678, TeA=0.7129,\n",
      "Epoch 3: TrL=0.5577, TrA=0.7180, VL=0.5495, VA=0.7273, TeL=0.5502, TeA=0.7268,\n",
      "Epoch 4: TrL=0.5363, TrA=0.7314, VL=0.5127, VA=0.7458, TeL=0.5133, TeA=0.7460,\n",
      "Epoch 5: TrL=0.5174, TrA=0.7425, VL=0.4914, VA=0.7627, TeL=0.4920, TeA=0.7631,\n",
      "Epoch 6: TrL=0.4957, TrA=0.7569, VL=0.6936, VA=0.6510, TeL=0.6947, TeA=0.6502,\n",
      "Epoch 7: TrL=0.4796, TrA=0.7658, VL=0.4793, VA=0.7672, TeL=0.4791, TeA=0.7676,\n",
      "Epoch 8: TrL=0.4643, TrA=0.7765, VL=0.4282, VA=0.7989, TeL=0.4286, TeA=0.7985,\n",
      "Epoch 9: TrL=0.4492, TrA=0.7850, VL=0.4400, VA=0.7875, TeL=0.4402, TeA=0.7879,\n",
      "Epoch 10: TrL=0.4373, TrA=0.7926, VL=0.4198, VA=0.8008, TeL=0.4202, TeA=0.8003,\n",
      "Epoch 11: TrL=0.4257, TrA=0.7992, VL=0.4153, VA=0.8039, TeL=0.4151, TeA=0.8045,\n",
      "Epoch 12: TrL=0.4167, TrA=0.8046, VL=0.4013, VA=0.8125, TeL=0.4014, TeA=0.8115,\n",
      "Epoch 13: TrL=0.4068, TrA=0.8080, VL=0.3935, VA=0.8170, TeL=0.3937, TeA=0.8170,\n",
      "Epoch 14: TrL=0.4010, TrA=0.8139, VL=0.3813, VA=0.8240, TeL=0.3816, TeA=0.8236,\n",
      "Epoch 15: TrL=0.3931, TrA=0.8166, VL=0.4747, VA=0.7780, TeL=0.4743, TeA=0.7793,\n",
      "Epoch 16: TrL=0.3871, TrA=0.8205, VL=0.3740, VA=0.8278, TeL=0.3740, TeA=0.8274,\n",
      "Epoch 17: TrL=0.3813, TrA=0.8235, VL=0.3547, VA=0.8381, TeL=0.3552, TeA=0.8372,\n",
      "Epoch 18: TrL=0.3763, TrA=0.8262, VL=0.3598, VA=0.8348, TeL=0.3595, TeA=0.8346,\n",
      "Epoch 19: TrL=0.3709, TrA=0.8288, VL=0.3556, VA=0.8377, TeL=0.3560, TeA=0.8369,\n",
      "Epoch 20: TrL=0.3650, TrA=0.8324, VL=0.3499, VA=0.8406, TeL=0.3497, TeA=0.8405,\n",
      "Epoch 21: TrL=0.3599, TrA=0.8347, VL=0.3646, VA=0.8321, TeL=0.3648, TeA=0.8315,\n",
      "Epoch 22: TrL=0.3576, TrA=0.8358, VL=0.3477, VA=0.8419, TeL=0.3476, TeA=0.8413,\n",
      "Epoch 23: TrL=0.3532, TrA=0.8387, VL=0.3263, VA=0.8538, TeL=0.3257, TeA=0.8527,\n",
      "Epoch 24: TrL=0.3476, TrA=0.8424, VL=0.3260, VA=0.8532, TeL=0.3255, TeA=0.8526,\n",
      "Epoch 25: TrL=0.3459, TrA=0.8416, VL=0.3317, VA=0.8494, TeL=0.3310, TeA=0.8500,\n",
      "Epoch 26: TrL=0.3387, TrA=0.8463, VL=0.3195, VA=0.8562, TeL=0.3187, TeA=0.8556,\n",
      "Epoch 27: TrL=0.3376, TrA=0.8454, VL=0.3304, VA=0.8521, TeL=0.3302, TeA=0.8518,\n"
     ]
    }
   ],
   "source": [
    "train(100, dev, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"./models/face_detection_model89\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random image\n",
    "base_path = \"../datasets/UCF-101-to-image/Utrain\"\n",
    "classification_path = \"../datasets/UCF-101-to-image-classified/Utrain\"\n",
    "\n",
    "image_paths = glob(os.path.join(base_path, \"*\"))\n",
    "for path in image_paths:\n",
    "    # Load image\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    # Show image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Transform image\n",
    "    img = test_transform(img)\n",
    "\n",
    "    # View as a batch\n",
    "    img.unsqueeze_(0)\n",
    "    # Copy to device\n",
    "    img = img.to(dev)\n",
    "    with torch.no_grad():\n",
    "        # Model forward\n",
    "        pred = model(img)\n",
    "    # Get prediction\n",
    "    _,pred = pred.max(1)\n",
    "    pred = pred.item()\n",
    "    print(f\"Predicted: {class_names[pred]}\")\n",
    "    if(pred == 0):\n",
    "        #copia in black\n",
    "        shutil.copy(path, classification_path + \"/black\")\n",
    "    else:\n",
    "        #copia in white\n",
    "        shutil.copy(path, classification_path + \"/white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "ML1819",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
